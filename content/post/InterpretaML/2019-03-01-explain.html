---
title: "Mejor Cajas Blancas"
author: "Romy Rodriguez-Ravines"
date: 2019-03-01
output:
  html_document:
    mathjax: default
    self_contained: yes
    toc: no
categories: ["R"]
tags: ["R", "Machine Learning", "Analytics", "Interpretability"]
---



<head>
<!-- use the font -->
<style type='text/css'>
    body {font-family: "Century Gothic", CenturyGothic, AppleGothic, sans-serif; 
    font-size: 12px;
    color: #9d9fa;}
    td { font-size: 16px;}
    h1 { font-family: "Century Gothic", CenturyGothic, AppleGothic, sans-serif;  font-size: 34px; color: #07077e;}
    h2 { font-family: "Century Gothic", CenturyGothic, AppleGothic, sans-serif; font-size: 30px; color: #004379;}
    h3 { font-family: "Century Gothic", CenturyGothic, AppleGothic, sans-serif; font-size: 28px; color: #00549a;}
    h4 { font-family: "Century Gothic", CenturyGothic, AppleGothic, sans-serif; font-size: 20px; color: #0069c0;}
   </style>
</head>
<div id="interpretando-modelos-de-machine-learning" class="section level2">
<h2>Interpretando Modelos de Machine Learning</h2>
<blockquote>
<p>Interpretación de modelos: habilidad para explicar y presentar un modelo en una forma comprensible para los seres humanos (Doshi and Been, 2017)</p>
</blockquote>
<p>Los modelos de Machine learning son conocidos por su alto poder predictivo, pero no por la facilidad de interpretación de sus resultados. Una regresión logística es un ejemplo modelo interpretable. Un modelo de <em>gradient boosting machine</em> o <em>gbm</em> es un ejemplo donde la interpretación no es trivial. Dada la importancia que tiene, desde la perspectiva de la toma de decisiones de negocio, explicar porque se considera que un cliente va a darse de baja o va a presentar una reclamación, hay un creciente número de publicaciones donde se presentan alternativas para abordar este tema.</p>
<p>La figura a continuación ilustra las diferencias en cuanto a interpretabilidad entre un modelo linear, preocupado por entender y prever un comportamiento medio, y un modelo de machine learning, preocupado por capturar todas las relaciones en los datos para garantizar una previsión más precisa. Forma general se puede decir que los modelos lineales son aproximados pero permiten una aproximación exacta, mientras que con machine learning se obtienen modelos muy bien entrenados (exatos) pero difíciles de interpretar.</p>
<p><img src='interpretaML/booklet01.png' alt=" " style="width:90%;"></p>
<div id="alcance-scope-de-la-interpretacion" class="section level3">
<h3>Alcance (Scope) de la Interpretación</h3>
<p>La interpretación de los modelos se puede realizar con dos perspectivas:</p>
<ol style="list-style-type: decimal">
<li><p>Interpretación <strong>GLOBAL</strong>. Permite explicar la interacción entre la variable respuesta (<em>target</em>) y las variables explicativas (<em>features</em>) utilizando el <strong>conjunto de datos completo</strong>.</p></li>
<li><p>Interpretación <strong>LOCAL</strong>. Permite explicar la interacción entre la variable respuesta (<em>target</em>) y las variables explicativas (<em>features</em>) para una única observación o <strong>individuo</strong>.</p></li>
</ol>
<p><img src='interpretaML/booklet02.png' alt=" " style="width:80%;"></p>
</div>
<div id="herramientas" class="section level3">
<h3>Herramientas</h3>
<p>Algunas de las herramientas disponibles para interpretar modelos <strong>caja negra</strong> son (no es una lista exhaustiva):</p>
<div id="interpretacion-global" class="section level4">
<h4>Interpretación Global</h4>
<ul>
<li><p><strong>Importancia de cada variable</strong> <em>Feature importance</em>:¿Cuáles son las variables (<em>features</em>) más importantes?</p></li>
<li><p><strong>Efecto de cada variable</strong>_Feature effects_: ¿Cómo influye una característica en la predicción? - Efectos locales acumulados (<em>ALE</em>), gráficos de dependencia parcial (<em>PDP</em>) y curvas de expectativa condicional individuales (<em>ICE</em>) -</p></li>
</ul>
</div>
<div id="interpretacion-local" class="section level4">
<h4>Interpretación Local</h4>
<ul>
<li><p><strong>Model-Agnostic</strong> <em>Explanations for single predictions</em>: ¿Cómo afectan los valores de cada variable a la predicción puntual? - Algunas opciones: <em>LIME</em>, <em>Shapley value</em> , <em>BreakDown</em>.</p></li>
<li><p><strong>Árboles sustitutos</strong> <em>Surrogate trees</em>: ¿Podemos aproximar el modelo de caja negra subyacente con un árbol de decisión corto?</p></li>
</ul>
</div>
</div>
<div id="opciones-en-r" class="section level3">
<h3>Opciones en R</h3>
<p>En <em>R</em> se dispone de varias opciones para aproximar “cajas blancas” a partir de las “cajas negras” (los modelos complejos de machine learning)</p>
<ul>
<li><p><strong>predict.xgb.Booster: Predict method for eXtreme Gradient Boosting model</strong> Permite calcular las aportaciones de cada característica a las previsiones individuales. Tiene dos opciones: calcular los valores <em>shap</em> o utilizar un árbol medio (aproximado)</p></li>
<li><p><strong>xgboostExplainer: makes XGBoost interpretable</strong><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> es un paquete de <em>R</em> que intenta hacer un modelo de <em>XGBoost</em> tan “transparente”" como el árbol de decisión único. La idea básica de este enfoque es sumar las contribuciones que tiene cada árbol ensamblado. Vale la pena señalar que el impacto de una característica o variable depende de la ruta específica que la observación toma a través del conjunto de árboles, por lo que no se puede asociar a un parámetro estático.</p></li>
<li><p><strong>iml: Interpretable Machine Learning</strong><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> es un paquete de <em>R</em> que proporciona varias herramientas para analizar y entender modelos de Machine Learning. En particular, para la interpretación de previsiones individuales ofrece tres opciones: <em>Lime</em>, <em>Shap</em> y <em>Surrogate trees</em>.</p></li>
<li><p><strong>DALEX: Descriptive mAchine Learning EXplanations</strong><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> es un paquete de <em>R</em> que proporciona un conjunto de herramientas que ayudan a comprender cómo funcionan los modelos complejos. Tiene dos aplicaciones: (1) Entender el modelo en sí mismo, y (2) Entender las previsiones individuales.</p></li>
</ul>
<p>Para ayudar en la interpretación de las previsiones indivuduales, DALEX hace uso de <em>Break Down: Model Agnostic Explainers for Individual Predictions</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. Se trata de un enfoque conocido como <em>model agnostic</em> porque usa modelos auxiliares (y no el propio modelo) para aproximar la interpretación de resultados.</p>
</div>
<div id="procedimiento-con-dalex" class="section level3">
<h3>Procedimiento con DALEX</h3>
<p>Todas las alternativas de interpretación de resultados individuales son computacionalmente costosas. Por ejemplo, usando <em>DALEX</em>, se requieren 2 minutos para desglosar una previsión basada en 20 variables. Sin embargo, se requieren 10 minutos para desglosar una única prevsión basada en 50 variables<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. En otras palabras, el tiempo necesario aumenta exponencialmente con la coplejidad del modelo.</p>
<p>Por dicho motivo, utilizamos un procedimiento aproximado para interpretar los resultados del modelo:</p>
<ol style="list-style-type: decimal">
<li>Clasificamos el TOP 2.5% de Clientes con mayor probabilidad de fuga en 15 <em>clusters</em> (grupos homogéneos) utilizando todas las características más importantes hasta el 75%<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>.</li>
<li>Utilizamos el <em>explainer</em> de <em>DALEX</em> para desglosar la previsión de cada uno de los clientes tipo (medoide) de cada cluster.</li>
<li>Asumimos que la explicatividad del medoide es extensible a todos los miembros de su grupo.</li>
</ol>
<p>Este enfoque ofrece ventajas como:</p>
<ol style="list-style-type: decimal">
<li>Ejecutar los procedimientos en tiempo asumibles en el entorno de producción.</li>
<li>Proporcionar una buena aproximación a la interpretación de la previsión individual de todos los clientes.</li>
<li>Proporcionar una interpretación agnóstica del modelo, priorizando las variables más importantes en la construcción de la previsión.</li>
</ol>
<p><img src='interpretaML/cluster1.png' alt=" " style="width:40%;">
<img src='interpretaML/cluster3.png' alt=" " style="width:40%;">
<img src='interpretaML/cluster10.png' alt=" " style="width:40%;">
<img src='interpretaML/cluster15.png' alt=" " style="width:40%;"></p>
</div>
<div id="procedimiento-con-xgboostexplainer" class="section level3">
<h3>Procedimiento con xgboostExplainer</h3>
<p>Esta opción es computacionalmente menos costosa y permite aproximar la interpretación de todas las previsiones de un lote de, por ejemplo, 7000 clientes en minutos. Además de ofrecer el peso de cada característica en cada previsión individual, ofrece un presentación gráfica muy similar a la proporcionada por DALEX.</p>
<p>Para facilitar la interpretación de resultados, se recomienda utilizar un modelo simplificado donde participen las variables que conttribuyan con, por ejemplo, el 75% del gain del modelo.</p>
<p><img src='interpretaML/booklet04.png' alt=" " style="width:40%;">
<img src='interpretaML/booklet03.png' alt=" " style="width:60%;"></p>
<p>Nota: Algunos recomiendan usar directamente la funición predict del xgboost con la opcion contrib=TRUE antes que las funciones del XGBoostExplainer. (ver aqui<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>)</p>
<div id="ejemplo" class="section level4">
<h4>Ejemplo</h4>
</div>
</div>
<div id="anexo" class="section level3">
<h3>Anexo</h3>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211" class="uri">https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p> <a href="https://cran.r-project.org/web/packages/iml/vignettes/intro.html" class="uri">https://cran.r-project.org/web/packages/iml/vignettes/intro.html</a><a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p><a href="https://pbiecek.github.io/DALEX_docs/" class="uri">https://pbiecek.github.io/DALEX_docs/</a><a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p><a href="https://pbiecek.github.io/breakDown/" class="uri">https://pbiecek.github.io/breakDown/</a><a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Aumentando las variables de 20 a 50 (x2.5), aumenta 5 veces el tiempo computacional.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>Tanto el número de clusters como el cut-off en el gain para seleccionar variables son parametrizables<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p><a href="https://www.reddit.com/r/learnmachinelearning/comments/9n2kq2/fyi_for_those_of_you_using_the_xgboostexplainer/" class="uri">https://www.reddit.com/r/learnmachinelearning/comments/9n2kq2/fyi_for_those_of_you_using_the_xgboostexplainer/</a><a href="#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
